<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Spark学习笔记 | IT小王</title><meta name="description" content="Spark学习笔记"><meta name="keywords" content="Spark"><meta name="author" content="IT小王"><meta name="copyright" content="IT小王"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.svg"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark学习笔记"><meta name="twitter:description" content="Spark学习笔记"><meta name="twitter:image" content="https://wangbowen.cn/postImages/SparkCover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Spark学习笔记"><meta property="og:url" content="https://wangbowen.cn/2020/04/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><meta property="og:site_name" content="IT小王"><meta property="og:description" content="Spark学习笔记"><meta property="og:image" content="https://wangbowen.cn/postImages/SparkCover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://wangbowen.cn/2020/04/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="prev" title="JAVA核心技术卷I（基本程序设计结构）" href="https://wangbowen.cn/2020/05/11/JAVA%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E5%8D%B7I%EF%BC%88%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84%EF%BC%89/"><link rel="next" title="CentOS7搭建Samba" href="https://wangbowen.cn/2020/04/23/CentOS7%E6%90%AD%E5%BB%BASamba/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"ELVDUY5WGR","apiKey":"55783f8f2945106559981cc355ad5d57","indexName":"hexoSearch","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"http://wangbowen.cn/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"top-center"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">IT小王</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">40</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">25</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">29</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Spark学习笔记"><span class="toc_mobile_items-text">Spark学习笔记</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#一、简介"><span class="toc_mobile_items-text">一、简介</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#二、部署"><span class="toc_mobile_items-text">二、部署</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-1-Local-模式"><span class="toc_mobile_items-text">2.1 Local 模式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-2-Standalone-模式"><span class="toc_mobile_items-text">2.2 Standalone 模式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-4-集成Hadoop-HA"><span class="toc_mobile_items-text">2.4 集成Hadoop HA</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-5-Spark-master-HA"><span class="toc_mobile_items-text">2.5 Spark master HA</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-6-Yarn-模式"><span class="toc_mobile_items-text">2.6 Yarn 模式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-7-mesos-模式"><span class="toc_mobile_items-text">2.7 mesos 模式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-8-启动脚本分析"><span class="toc_mobile_items-text">2.8 启动脚本分析</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#三、快速入门"><span class="toc_mobile_items-text">三、快速入门</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-1-主要对象"><span class="toc_mobile_items-text">3.1 主要对象</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-2-编程单词计数程序-Standalone模式"><span class="toc_mobile_items-text">3.2 编程单词计数程序(Standalone模式)</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3-2-1-Scala"><span class="toc_mobile_items-text">3.2.1 Scala</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3-2-2-Java"><span class="toc_mobile_items-text">3.2.2 Java</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#四、RDD-弹性分布式数据集"><span class="toc_mobile_items-text">四、RDD 弹性分布式数据集</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-1-并发度"><span class="toc_mobile_items-text">4.1 并发度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-2-RDD-变换"><span class="toc_mobile_items-text">4.2 RDD 变换</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-3-RDD-动作"><span class="toc_mobile_items-text">4.3 RDD 动作</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#五、Spark核心API"><span class="toc_mobile_items-text">五、Spark核心API</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#六、Spark任务提交流程"><span class="toc_mobile_items-text">六、Spark任务提交流程</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#七、依赖"><span class="toc_mobile_items-text">七、依赖</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#八、持久化"><span class="toc_mobile_items-text">八、持久化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#九、共享变量"><span class="toc_mobile_items-text">九、共享变量</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-1-广播变量"><span class="toc_mobile_items-text">9.1 广播变量</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#9-2-累加器"><span class="toc_mobile_items-text">9.2 累加器</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#十、Spark-SQL"><span class="toc_mobile_items-text">十、Spark SQL</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-1-Scala-版"><span class="toc_mobile_items-text">10.1 Scala 版</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-1-1-创建-DataFrame-收据框"><span class="toc_mobile_items-text">10.1.1 创建 DataFrame 收据框</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-1-2-创建临时视图"><span class="toc_mobile_items-text">10.1.2 创建临时视图</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-1-3-查询数据"><span class="toc_mobile_items-text">10.1.3 查询数据</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-2-Java-版"><span class="toc_mobile_items-text">10.2 Java 版</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-2-1-处理-json-数据"><span class="toc_mobile_items-text">10.2.1 处理 json 数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-2-2-DataSet-转-RDD"><span class="toc_mobile_items-text">10.2.2 DataSet 转 RDD</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-2-3-处理-jdbc-数据"><span class="toc_mobile_items-text">10.2.3 处理 jdbc 数据</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-3-整合-Hive"><span class="toc_mobile_items-text">10.3 整合 Hive</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-3-1-linux环境"><span class="toc_mobile_items-text">10.3.1 linux环境</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-3-2-IDEA环境（JAVA）"><span class="toc_mobile_items-text">10.3.2 IDEA环境（JAVA）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-3-3-可能遇到的问题"><span class="toc_mobile_items-text">10.3.3 可能遇到的问题</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-4-SQL查询引擎"><span class="toc_mobile_items-text">10.4 SQL查询引擎</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#10-5-拓展和总结"><span class="toc_mobile_items-text">10.5 拓展和总结</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-1-使用场景"><span class="toc_mobile_items-text">10.5.1 使用场景</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-2-加载数据"><span class="toc_mobile_items-text">10.5.2 加载数据</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-3-DataFrame函数-和-SQL"><span class="toc_mobile_items-text">10.5.3 DataFrame函数 和 SQL</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-4-Schema"><span class="toc_mobile_items-text">10.5.4 Schema</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-5-加载和保存结果"><span class="toc_mobile_items-text">10.5.5 加载和保存结果</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-6-SQL函数覆盖"><span class="toc_mobile_items-text">10.5.6 SQL函数覆盖</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-7-复杂JSON处理"><span class="toc_mobile_items-text">10.5.7 复杂JSON处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#10-5-8-外部数据源"><span class="toc_mobile_items-text">10.5.8 外部数据源</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#十一、Spark-Streaming"><span class="toc_mobile_items-text">十一、Spark Streaming</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-1-介绍"><span class="toc_mobile_items-text">11.1 介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-2-快速入门-单词计数"><span class="toc_mobile_items-text">11.2 快速入门(单词计数)</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-2-1-Scala-本地"><span class="toc_mobile_items-text">11.2.1 Scala + 本地</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-2-2-Java-集群"><span class="toc_mobile_items-text">11.2.2 Java + 集群</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-3-DStream-和-Receiver"><span class="toc_mobile_items-text">11.3 DStream 和 Receiver</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-3-1-DSteam"><span class="toc_mobile_items-text">11.3.1 DSteam</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-3-2-Receiver"><span class="toc_mobile_items-text">11.3.2 Receiver</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-4-Kafka-集成"><span class="toc_mobile_items-text">11.4 Kafka 集成</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-5-状态更新"><span class="toc_mobile_items-text">11.5 状态更新</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-5-1-updateStateByKey"><span class="toc_mobile_items-text">11.5.1 updateStateByKey</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-5-2-mapWithState"><span class="toc_mobile_items-text">11.5.2 mapWithState</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-5-3-代码示例"><span class="toc_mobile_items-text">11.5.3 代码示例</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-6-窗口化操作"><span class="toc_mobile_items-text">11.6 窗口化操作</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-7-容错处理"><span class="toc_mobile_items-text">11.7 容错处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-7-1-生产环境中spark-streaming的job的注意事项"><span class="toc_mobile_items-text">11.7.1 生产环境中spark streaming的job的注意事项</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#11-7-2-spark-streaming中的容错实现"><span class="toc_mobile_items-text">11.7.2 spark streaming中的容错实现</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#11-8-将结果写入MySQL"><span class="toc_mobile_items-text">11.8 将结果写入MySQL</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#十二、SparkApp-部署模式"><span class="toc_mobile_items-text">十二、SparkApp 部署模式</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#12-1-闭包"><span class="toc_mobile_items-text">12.1 闭包</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#12-2-client"><span class="toc_mobile_items-text">12.2 client</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#12-3-cluster"><span class="toc_mobile_items-text">12.3 cluster</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#十三、Spark集成Hive查询Hbase"><span class="toc_mobile_items-text">十三、Spark集成Hive查询Hbase</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#13-1-local-模式-spark-shell"><span class="toc_mobile_items-text">13.1 local 模式 + spark-shell</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#13-2-standalone-模式-spark-shell"><span class="toc_mobile_items-text">13.2 standalone 模式 + spark-shell</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#13-3-IDEA编程访问"><span class="toc_mobile_items-text">13.3 IDEA编程访问</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#13-4-整合常见问题"><span class="toc_mobile_items-text">13.4 整合常见问题</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#十四、优化"><span class="toc_mobile_items-text">十四、优化</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#14-1-存储格式的选择"><span class="toc_mobile_items-text">14.1 存储格式的选择</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#14-2-压缩格式"><span class="toc_mobile_items-text">14.2 压缩格式</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#14-3-代码优化"><span class="toc_mobile_items-text">14.3 代码优化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#14-4-参数优化"><span class="toc_mobile_items-text">14.4 参数优化</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark学习笔记"><span class="toc-text">Spark学习笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、简介"><span class="toc-text">一、简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、部署"><span class="toc-text">二、部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Local-模式"><span class="toc-text">2.1 Local 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Standalone-模式"><span class="toc-text">2.2 Standalone 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-集成Hadoop-HA"><span class="toc-text">2.4 集成Hadoop HA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Spark-master-HA"><span class="toc-text">2.5 Spark master HA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Yarn-模式"><span class="toc-text">2.6 Yarn 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-mesos-模式"><span class="toc-text">2.7 mesos 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-启动脚本分析"><span class="toc-text">2.8 启动脚本分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、快速入门"><span class="toc-text">三、快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-主要对象"><span class="toc-text">3.1 主要对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-编程单词计数程序-Standalone模式"><span class="toc-text">3.2 编程单词计数程序(Standalone模式)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Scala"><span class="toc-text">3.2.1 Scala</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Java"><span class="toc-text">3.2.2 Java</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四、RDD-弹性分布式数据集"><span class="toc-text">四、RDD 弹性分布式数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-并发度"><span class="toc-text">4.1 并发度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-RDD-变换"><span class="toc-text">4.2 RDD 变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-RDD-动作"><span class="toc-text">4.3 RDD 动作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五、Spark核心API"><span class="toc-text">五、Spark核心API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#六、Spark任务提交流程"><span class="toc-text">六、Spark任务提交流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#七、依赖"><span class="toc-text">七、依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#八、持久化"><span class="toc-text">八、持久化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#九、共享变量"><span class="toc-text">九、共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-广播变量"><span class="toc-text">9.1 广播变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-累加器"><span class="toc-text">9.2 累加器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#十、Spark-SQL"><span class="toc-text">十、Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-Scala-版"><span class="toc-text">10.1 Scala 版</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-1-创建-DataFrame-收据框"><span class="toc-text">10.1.1 创建 DataFrame 收据框</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-2-创建临时视图"><span class="toc-text">10.1.2 创建临时视图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-1-3-查询数据"><span class="toc-text">10.1.3 查询数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-Java-版"><span class="toc-text">10.2 Java 版</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-2-1-处理-json-数据"><span class="toc-text">10.2.1 处理 json 数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-2-2-DataSet-转-RDD"><span class="toc-text">10.2.2 DataSet 转 RDD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-2-3-处理-jdbc-数据"><span class="toc-text">10.2.3 处理 jdbc 数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-整合-Hive"><span class="toc-text">10.3 整合 Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-3-1-linux环境"><span class="toc-text">10.3.1 linux环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-3-2-IDEA环境（JAVA）"><span class="toc-text">10.3.2 IDEA环境（JAVA）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-3-3-可能遇到的问题"><span class="toc-text">10.3.3 可能遇到的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-SQL查询引擎"><span class="toc-text">10.4 SQL查询引擎</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-拓展和总结"><span class="toc-text">10.5 拓展和总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-1-使用场景"><span class="toc-text">10.5.1 使用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-2-加载数据"><span class="toc-text">10.5.2 加载数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-3-DataFrame函数-和-SQL"><span class="toc-text">10.5.3 DataFrame函数 和 SQL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-4-Schema"><span class="toc-text">10.5.4 Schema</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-5-加载和保存结果"><span class="toc-text">10.5.5 加载和保存结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-6-SQL函数覆盖"><span class="toc-text">10.5.6 SQL函数覆盖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-7-复杂JSON处理"><span class="toc-text">10.5.7 复杂JSON处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-5-8-外部数据源"><span class="toc-text">10.5.8 外部数据源</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#十一、Spark-Streaming"><span class="toc-text">十一、Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-介绍"><span class="toc-text">11.1 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-快速入门-单词计数"><span class="toc-text">11.2 快速入门(单词计数)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-2-1-Scala-本地"><span class="toc-text">11.2.1 Scala + 本地</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-2-2-Java-集群"><span class="toc-text">11.2.2 Java + 集群</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-DStream-和-Receiver"><span class="toc-text">11.3 DStream 和 Receiver</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-1-DSteam"><span class="toc-text">11.3.1 DSteam</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-2-Receiver"><span class="toc-text">11.3.2 Receiver</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-Kafka-集成"><span class="toc-text">11.4 Kafka 集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-5-状态更新"><span class="toc-text">11.5 状态更新</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-5-1-updateStateByKey"><span class="toc-text">11.5.1 updateStateByKey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-5-2-mapWithState"><span class="toc-text">11.5.2 mapWithState</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-5-3-代码示例"><span class="toc-text">11.5.3 代码示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-窗口化操作"><span class="toc-text">11.6 窗口化操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-7-容错处理"><span class="toc-text">11.7 容错处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-7-1-生产环境中spark-streaming的job的注意事项"><span class="toc-text">11.7.1 生产环境中spark streaming的job的注意事项</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-7-2-spark-streaming中的容错实现"><span class="toc-text">11.7.2 spark streaming中的容错实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-8-将结果写入MySQL"><span class="toc-text">11.8 将结果写入MySQL</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#十二、SparkApp-部署模式"><span class="toc-text">十二、SparkApp 部署模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-闭包"><span class="toc-text">12.1 闭包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-client"><span class="toc-text">12.2 client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-cluster"><span class="toc-text">12.3 cluster</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#十三、Spark集成Hive查询Hbase"><span class="toc-text">十三、Spark集成Hive查询Hbase</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#13-1-local-模式-spark-shell"><span class="toc-text">13.1 local 模式 + spark-shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-2-standalone-模式-spark-shell"><span class="toc-text">13.2 standalone 模式 + spark-shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-3-IDEA编程访问"><span class="toc-text">13.3 IDEA编程访问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-4-整合常见问题"><span class="toc-text">13.4 整合常见问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#十四、优化"><span class="toc-text">十四、优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#14-1-存储格式的选择"><span class="toc-text">14.1 存储格式的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-2-压缩格式"><span class="toc-text">14.2 压缩格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-3-代码优化"><span class="toc-text">14.3 代码优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-4-参数优化"><span class="toc-text">14.4 参数优化</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/postImages/SparkCover.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Spark学习笔记</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-04-28<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-04-28</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/">Spark</a></span><div class="post-meta-wordcount"><div class="post-meta-pv-cv"><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="Spark学习笔记"><a href="#Spark学习笔记" class="headerlink" title="Spark学习笔记"></a>Spark学习笔记</h1><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Lightning-fast cluster computing。</p>
<p>快如闪电的集群计算。</p>
<p>大规模快速通用的计算引擎。</p>
<p>速度：比hadoop 100x,磁盘计算快10x</p>
<p>使用：java / Scala /R /python</p>
<p>​            提供80+算子(操作符)，容易构建并行应用。</p>
<p>通用：组合SQL ，流计算 + 复杂分析。</p>
<p>运行：Hadoop, Mesos, standalone, or in the cloud,local.</p>
<p><strong>模块</strong></p>
<ul>
<li>Spark core：核心模块。通用执行引擎，提供内存计算和对外部数据集的引用。</li>
<li>Spark SQL：构建在core之上，引入新的抽象SchemaRDD，提供了结构化和半结构化支持。</li>
<li>Spark Streaming：流计算。小批量计算，RDD.</li>
<li>Spark MLlib：机器学习库。</li>
<li>Spark graph：图计算</li>
</ul>
<h2 id="二、部署"><a href="#二、部署" class="headerlink" title="二、部署"></a>二、部署</h2><p><strong>本地开发环境版本，一定要和spark运行环境一致！！！！！</strong></p>
<h3 id="2-1-Local-模式"><a href="#2-1-Local-模式" class="headerlink" title="2.1 Local 模式"></a>2.1 Local 模式</h3><p>local 模式 ：通过多线程模拟分布式计算。</p>
<p><strong>部署流程</strong></p>
<ol>
<li><p>下载 <strong>spark-2.4.3-bin-hadoop2.7.tgz</strong></p>
</li>
<li><p>解压</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz -C &#x2F;soft&#x2F;</span><br><span class="line">$ln -s spark-2.4.3-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>配置环境变量</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># spark</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;soft&#x2F;spark</span><br><span class="line">export PATH&#x3D;$PATH:$SPARK_HOME&#x2F;bin:$SPARK_HOME&#x2F;sbin</span><br><span class="line"></span><br><span class="line">$source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>验证spark</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark&#x2F;bin]$.&#x2F;spark-shell</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>WebUI</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;s201:4040&#x2F;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>在IDEA中编写代码运行</p>
<blockquote>
<p><strong>注意：Spark2.4.3 对应 scala的SDK版本是2.11.12，以及pom依赖是spark-core_2.11。其他版本查看/soft/spark/jars/scala-compiler-X.jar和spark-core_X.jar。</strong></p>
</blockquote>
<ol>
<li><p>添加MAVEN工程，添加SCALA支持，添加pom依赖</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>
</li>
<li><p>编写代码并运行</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建spark配置对象.【本地模式】</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">// 加载文本文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(<span class="string">"D:\\tmp\\1.txt"</span>)</span><br><span class="line">    <span class="comment">// 分割并炸开</span></span><br><span class="line">    rdd.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">      <span class="comment">// 映射二元组</span></span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      <span class="comment">// 根据Key分组聚合</span></span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      <span class="comment">// 返回数组结果</span></span><br><span class="line">      .collect()</span><br><span class="line">      <span class="comment">// 打印结果</span></span><br><span class="line">      .foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
</li>
</ol>
<h3 id="2-2-Standalone-模式"><a href="#2-2-Standalone-模式" class="headerlink" title="2.2 Standalone 模式"></a>2.2 Standalone 模式</h3><p>Standalone即Spark独立集群，由maste（分任务发到worker,相当于Yarn的RM）和worker（计算，返回结果。相当于Yarn的NM）组成。是Spark自带的资源调度（相当于Yarn）。因此可以脱离yarn独立存在，也可以理解为standalone替代了yarn的工作（后面会讲到spark on yarn）【也可以说是，MR作业交给yarn的RM，这里Spark作业交给Standalone的master】</p>
<p><strong>部署流程</strong></p>
<ol>
<li><p>规划S201为MASTE节点，S202~S204为slave节点，在S201上以本地模式安装好spark。</p>
</li>
<li><p>配置master节点的slaves配置文件</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark&#x2F;conf]$cp slaves.template slaves</span><br><span class="line">$vi slaves</span><br><span class="line"></span><br><span class="line"># 清空所有内容添加以下内容</span><br><span class="line">s202</span><br><span class="line">s203</span><br><span class="line">s204</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>可能要去<strong>spark-env.sh</strong>配置一下JAVA_HOME</p>
</li>
<li><p>分发到s202~s204，设置好连接以及，环境变量。</p>
</li>
<li><p>启动集群</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark&#x2F;sbin]$.&#x2F;start-all.sh</span><br></pre></td></tr></table></figure></div>

<p><strong>注意：</strong></p>
<ol>
<li><p>要加上<strong>./</strong>，不然会与HDFS的start-all脚本冲突！</p>
</li>
<li><p>可能会遇到启动失败问题</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s204:   JAVA_HOME is not set</span><br><span class="line">s204: full log in &#x2F;soft&#x2F;spark&#x2F;logs&#x2F;spark-wbw-org.apache.spark.deploy.worker.Worker-1-s204.out</span><br></pre></td></tr></table></figure></div>

<p>原因是：spark不能读取/etc/profile导致，在<strong>sbin/spark-env.sh</strong>里面配置JAVA_HOME：<strong>export JAVA_HOME=/soft/jdk</strong></p>
</li>
</ol>
</li>
<li><p>查看进程</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark&#x2F;sbin]$xcall.sh jps</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; s201 : jps &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">1737 QuorumPeerMain</span><br><span class="line">2938 Jps</span><br><span class="line">2811 Master</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; s202 : jps &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">1824 Worker</span><br><span class="line">1897 Jps</span><br><span class="line">1309 QuorumPeerMain</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; s203 : jps &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">1866 Worker</span><br><span class="line">1915 Jps</span><br><span class="line">1359 QuorumPeerMain</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; s204 : jps &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">1810 Worker</span><br><span class="line">1859 Jps</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; s205 : jps &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">1526 Jps</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看WEBUI</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;s201:8080&#x2F;</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="2-4-集成Hadoop-HA"><a href="#2-4-集成Hadoop-HA" class="headerlink" title="2.4 集成Hadoop HA"></a>2.4 集成Hadoop HA</h3><p>使spark集群可以访问HDFS。</p>
<p><strong>集成流程</strong></p>
<ol>
<li><p>复制<strong>core-site.xml *<em>和 *</em>hdfs-site.xml</strong>到spark/conf目录下</p>
</li>
<li><p>分发到所有集群节点</p>
</li>
<li><p>启动spark集群</p>
</li>
<li><p>启动spark-shell,连接spark集群上</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&gt;spark-shell --master spark:&#x2F;&#x2F;s201:7077</span><br><span class="line">$scala&gt;sc.textFile(&quot;hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;centos&#x2F;test.txt&quot;).collect();</span><br></pre></td></tr></table></figure></div>




</li>
</ol>
<h3 id="2-5-Spark-master-HA"><a href="#2-5-Spark-master-HA" class="headerlink" title="2.5 Spark master HA"></a>2.5 Spark master HA</h3><p>master的高可用配置，和Hadoop一样如果只有一个NameNode会造成单点故障的问题。</p>
<p>只针对standalone和mesos集群部署情况。</p>
<p>使用zk连接多个master并存储state。</p>
<p><strong>配置流程</strong></p>
<ol>
<li><p>配置<strong>spark/conf/spark-env.sh</strong>，在末尾添加</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=s201:2181,s202:2181,s203:2181 -Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>分发配置，重启spark集群</p>
</li>
<li><p>在s204上单独开启master</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s204 &#x2F;soft&#x2F;spark&#x2F;sbin]$.&#x2F;start-master.sh</span><br></pre></td></tr></table></figure></div>

<p>查看8080端口WEB，可以看到<strong>s201</strong>处于<strong>ALIVE</strong>，而<strong>s204</strong>处于<strong>STANDBY</strong></p>
<p><a href="/postImages/sparkHA.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/sparkHA.png" class="lazyload"></a></p>
<p><a href="/postImages/sparkHA2.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/sparkHA2.png" class="lazyload"></a></p>
</li>
<li><p>杀掉s201上的master进程</p>
</li>
<li><p>再次通过WEB查看状态（可能要1~2分钟才切过去）</p>
<p><a href="/postImages/sparkHA3.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/sparkHA3.png" class="lazyload"></a></p>
</li>
</ol>
<h3 id="2-6-Yarn-模式"><a href="#2-6-Yarn-模式" class="headerlink" title="2.6 Yarn 模式"></a>2.6 Yarn 模式</h3><p>在yarn作为资源管理器的情况下，一个spark应用只是一个yarn下的应用而已，类似于mr一样，所以不需要启动master和worker。应用提交到yarn后，首先会申请一个container运行applicationmaster，而这个appmaster里运行的就是spark的driver，对应的executor也在container里运行。提交应用需要带上spark-assembly的jar包，包可以位于hdfs上，即使位于本地，也会被上传到hdfs进行分发。</p>
<p><strong>部署流程</strong></p>
<ol>
<li><p>将spark的jars文件放到hdfs上</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark]$hdfs dfs -mkdir -p &#x2F;spark&#x2F;jars</span><br><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark]$hdfs dfs -put jars&#x2F;* &#x2F;spark&#x2F;jars</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>配置spark配置文件</p>
<ul>
<li><p>/conf/spark-default.conf</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$cp spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">$vi spark-defaults.conf</span><br></pre></td></tr></table></figure></div>

<p>直接在文件末尾添加如下内容（spark的jars在HDFS上的位置，也可以在提交的时候加–jars 以及 –files）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.yarn.jars&#x3D;hdfs:&#x2F;&#x2F;mycluster&#x2F;spark&#x2F;jars&#x2F;*</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>/conf/spark-env.sh</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$cp spark-env.sh.template spark-env.sh</span><br><span class="line">$vi spark-env.sh</span><br></pre></td></tr></table></figure></div>

<p>直接在文件末尾添加如下内容</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;soft&#x2F;jdk</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;soft&#x2F;hadoop&#x2F;etc&#x2F;hadoop</span><br></pre></td></tr></table></figure></div>
</li>
</ul>
</li>
<li><p>分发<strong>spark-default.conf *<em>和 *</em>spark-env.sh</strong> 到所有节点上。重启spark集群。(这里只要提交作业的那个机器就可以了)</p>
</li>
<li><p>提交作业（master改为yarn，即作业交给yarn来管理）</p>
<ul>
<li><p>cluster 模式</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.scala.SubmitDeployMode --master yarn --deploy-mode cluster hdfs:&#x2F;&#x2F;mycluster&#x2F;demoJars&#x2F;Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>client 模式</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.scala.SubmitDeployMode --master yarn --deploy-mode client Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>



</li>
</ul>
</li>
</ol>
<p>   <strong>注意：如果是用虚拟机搭建，可能会由于虚拟机内存过小而导致启动失败，比如内存资源过小，yarn会直接kill掉进程导致rpc连接失败。所以，我们还需要配置Hadoop的yarn-site.xml文件，加入如下两项配置：</strong></p>
   <div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></div>



<h3 id="2-7-mesos-模式"><a href="#2-7-mesos-模式" class="headerlink" title="2.7 mesos 模式"></a>2.7 mesos 模式</h3><p>Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。</p>
<h3 id="2-8-启动脚本分析"><a href="#2-8-启动脚本分析" class="headerlink" title="2.8 启动脚本分析"></a>2.8 启动脚本分析</h3><p><strong>start-all.sh</strong>：启动所有脚本</p>
<ul>
<li>sbin/spark-config.sh：读取配置文件</li>
<li>sbin/start-master.sh：启动master进程<ul>
<li>sbin/spark-config.sh<ul>
<li>export JAVA_HOME 配置一些环境</li>
</ul>
</li>
<li>bin/load-spark-env.sh<ul>
<li>if [ -f “${SPARK_CONF_DIR}/spark-env.sh” ];</li>
</ul>
</li>
<li>sbin/spark-daemon.sh </li>
</ul>
</li>
<li>sbin/start-slaves.sh：启动worker进程<ul>
<li>sbin/spark-config.sh</li>
<li>bin/load-spark-env.sh</li>
<li>sbin/slaves.sh</li>
<li>sbin/start-slave.sh<ul>
<li>sbin/spark-config.sh</li>
<li>bin/load-spark-env.sh</li>
<li>sbin/spark-daemon.sh</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="三、快速入门"><a href="#三、快速入门" class="headerlink" title="三、快速入门"></a>三、快速入门</h2><h3 id="3-1-主要对象"><a href="#3-1-主要对象" class="headerlink" title="3.1 主要对象"></a>3.1 主要对象</h3><ul>
<li>SparkContext：Spark程序的入口点，封装了整个spark运行环境的信息。</li>
<li>RDD：resilient distributed dataset,弹性分布式数据集。等价于集合。</li>
</ul>
<h3 id="3-2-编程单词计数程序-Standalone模式"><a href="#3-2-编程单词计数程序-Standalone模式" class="headerlink" title="3.2 编程单词计数程序(Standalone模式)"></a>3.2 编程单词计数程序(Standalone模式)</h3><h4 id="3-2-1-Scala"><a href="#3-2-1-Scala" class="headerlink" title="3.2.1 Scala"></a>3.2.1 Scala</h4><ol>
<li><p>添加scala编译插件（不然打出的jar包没有sacla的类）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">recompileMode</span>&gt;</span>incremental<span class="tag">&lt;/<span class="name">recompileMode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure></div>
</li>
<li><p>编写Scala文件</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建spark配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">    <span class="comment">// 本地模式</span></span><br><span class="line">    <span class="comment">//conf.setMaster("local")</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">// 加载文本文件</span></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line">    <span class="comment">// 分割并炸开</span></span><br><span class="line">    rdd.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">      <span class="comment">// 映射二元组</span></span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      <span class="comment">// 根据Key分组聚合</span></span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      <span class="comment">// 返回数组结果</span></span><br><span class="line">      .collect()</span><br><span class="line">      <span class="comment">// 打印结果</span></span><br><span class="line">      .foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>MAVEN打包，上传</p>
</li>
<li><p>创建一个文本文件，并上传到HDFS</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$vi wordcount.txt</span><br><span class="line">hello world</span><br><span class="line">hello scala</span><br><span class="line">hello spark</span><br><span class="line">hello spark</span><br><span class="line"></span><br><span class="line">$start-dfs.sh</span><br><span class="line">$hdfs dfs -put wordcount.txt &#x2F;tmp&#x2F;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>运行作业（注意哪个namenode是激活态用哪个，这里是s201）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --master spark:&#x2F;&#x2F;s201:7077 --class cn.wangbowen.spark.scala.WordCount Spark-1.0-SNAPSHOT.jar hdfs:&#x2F;&#x2F;s201:8020&#x2F;tmp&#x2F;wordcount.txt</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h4 id="3-2-2-Java"><a href="#3-2-2-Java" class="headerlink" title="3.2.2 Java"></a>3.2.2 Java</h4><ol>
<li><p>编写代码</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * WordCount class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/4/29 22:06</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 创建SparkConf对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setAppName(<span class="string">"WordCount"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建JAVA SC</span></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">// 加载文本</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd = sc.textFile(args[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 压扁。这里的泛型，第一个都是输入类型，后面跟输出类型</span></span><br><span class="line">        JavaRDD&lt;String&gt; rdd1 = rdd.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = s.split(<span class="string">" "</span>);</span><br><span class="line">                List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;(Arrays.asList(words));</span><br><span class="line">                <span class="keyword">return</span> list.iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 映射</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; rdd2 = rdd1.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// reduce</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; rdd3 = rdd2.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> integer + integer2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = rdd3.collect();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; tuple2 : collect) &#123;</span><br><span class="line">            System.out.println(tuple2._1 + <span class="string">": "</span> + tuple2._2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>MAVEN打包，上传</p>
</li>
<li><p>运行</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --master spark:&#x2F;&#x2F;s201:7077 --class cn.wangbowen.spark.java.WordCount Spark-1.0-SNAPSHOT.jar hdfs:&#x2F;&#x2F;s201:8020&#x2F;tmp&#x2F;wordcount.txt</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h2 id="四、RDD-弹性分布式数据集"><a href="#四、RDD-弹性分布式数据集" class="headerlink" title="四、RDD 弹性分布式数据集"></a>四、RDD 弹性分布式数据集</h2><p>是spark的基本数据结构，是不可变数据集。RDD中的数据集进行逻辑分区，每个分区可以单独在集群节点进行计算。可以包含任何java,scala，python和自定义类型。</p>
<ul>
<li><p>RDD是只读的记录分区集合。</p>
</li>
<li><p>RDD具有容错机制。</p>
</li>
<li><p>创建RDD方式：</p>
<ol>
<li>并行化一个现有集合。</li>
<li>外部存储。</li>
</ol>
</li>
<li><p>内存处理计算。在job间进行数据共享。内存的IO速率高于网络和disk的10 ~ 100之间（hadoop 花费90%时间用户rw）。</p>
</li>
<li><p>内部包含5个主要属性：</p>
<ol>
<li>分区列表</li>
<li>针对每个split的计算函数。</li>
<li>对其他rdd的依赖列表</li>
<li>可选，如果是KeyValueRDD的话，可以带分区类。</li>
<li>可选，首选块位置列表(hdfs block location);</li>
</ol>
</li>
</ul>
<h3 id="4-1-并发度"><a href="#4-1-并发度" class="headerlink" title="4.1 并发度"></a>4.1 并发度</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 5.</span><br><span class="line">local.backend.defaultParallelism() &#x3D; scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br><span class="line">&#x2F;&#x2F; 4.</span><br><span class="line">taskScheduler.defaultParallelism &#x3D; backend.defaultParallelism()</span><br><span class="line">&#x2F;&#x2F; 3.</span><br><span class="line">sc.defaultParallelism &#x3D;...; taskScheduler.defaultParallelism</span><br><span class="line">&#x2F;&#x2F; 2.取最小值</span><br><span class="line">defaultMinPartitions &#x3D; math.min(defaultParallelism, 2)</span><br><span class="line">&#x2F;&#x2F; 1.输入文本文件，第二个参数是并发度</span><br><span class="line">sc.textFile(path,defaultMinPartitions)			&#x2F;&#x2F;1,2</span><br></pre></td></tr></table></figure></div>



<h3 id="4-2-RDD-变换"><a href="#4-2-RDD-变换" class="headerlink" title="4.2 RDD 变换"></a>4.2 RDD 变换</h3><p>返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。</p>
<table>
<thead>
<tr>
<th>函数名</th>
<th>参数</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>map</td>
<td>(T) =&gt; V</td>
<td>对每个元素进行变换，应用变换函数。即映射，将数据集中的每个元素进行变化出新的元素。</td>
</tr>
<tr>
<td>filter</td>
<td>(T) =&gt; Boolean</td>
<td>过滤器。过滤掉返回false的数据。</td>
</tr>
<tr>
<td>flatMap</td>
<td>(T) =&gt; TraversableOnce[U]</td>
<td>压扁。对于返回的一组数据，分别拆分成单个数据。</td>
</tr>
<tr>
<td>mapPartitions</td>
<td>Iterator[T] =&gt; Iterator[U]</td>
<td>对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。迭代器内容是该分区里的数据集。</td>
</tr>
<tr>
<td>mapPartitionsWithIndex</td>
<td>(Int, Iterator[T]) =&gt; Iterator[U]</td>
<td>同上，可以指定分区。</td>
</tr>
<tr>
<td>sample</td>
<td>withReplacement: Boolean,fraction: Double,seed: Long = {}</td>
<td>采样。返回采样的RDD子集。</td>
</tr>
<tr>
<td>union</td>
<td>RDD[T]</td>
<td>类似于mysql union操作【select * from persons where id &lt; 10 union select * from id persons where id &gt; 29 ;】</td>
</tr>
<tr>
<td>intersection</td>
<td>RDD[T]</td>
<td>交集。提取两个rdd中都含有的元素。</td>
</tr>
<tr>
<td>distinct</td>
<td>可选[numTasks]</td>
<td>去重。去除重复的元素。</td>
</tr>
<tr>
<td>groupByKey</td>
<td>(K,V) =&gt; (K,Iterable[V])</td>
<td>根据Key分组，相同Key的值被放入一个迭代器。</td>
</tr>
<tr>
<td>reduceByKey</td>
<td>(V, V) =&gt; V</td>
<td>按key聚合。</td>
</tr>
<tr>
<td>sortByKey</td>
<td></td>
<td>排序</td>
</tr>
<tr>
<td>join</td>
<td>RDD[T], [numTasks]</td>
<td>连接。(K,V).join(K,W) =&gt;(K,(V,W))。只会对两个RDD相同Key的数据进行合并成一个元组，一个RDD有，而另一个没有的，不会进行连接。</td>
</tr>
<tr>
<td>cogroup</td>
<td>RDD[T]</td>
<td>协分组。对标JOIN，将相同KEY分到一组。(K,V).cogroup(K,W) =&gt;(K, (Iterable[V], Iterable[W]))</td>
</tr>
<tr>
<td>cartesian</td>
<td>RDD[T]</td>
<td>笛卡尔积。RDD[T] RDD[U] =&gt; RDD[(T,U)]</td>
</tr>
<tr>
<td>pipe</td>
<td>cmd:String</td>
<td>将rdd的元素传递给脚本或者命令，执行结果返回形成新的RDD</td>
</tr>
<tr>
<td>coalesce</td>
<td>numPartitions:Int</td>
<td>减少分区</td>
</tr>
<tr>
<td>repartition</td>
<td></td>
<td>可增可减</td>
</tr>
</tbody></table>
<h3 id="4-3-RDD-动作"><a href="#4-3-RDD-动作" class="headerlink" title="4.3 RDD 动作"></a>4.3 RDD 动作</h3><p>一个RDD数据集经过RDD变换不会立即执行，只有遇到RDD动作的时候才会开始执行。</p>
<table>
<thead>
<tr>
<th>函数名</th>
<th>参数</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>collect</td>
<td></td>
<td>收集rdd元素形成数组。</td>
</tr>
<tr>
<td>count</td>
<td></td>
<td>统计rdd元素的个数</td>
</tr>
<tr>
<td>reduce</td>
<td></td>
<td>聚合,返回一个值。</td>
</tr>
<tr>
<td>first</td>
<td></td>
<td>取出第一个元素take(1)</td>
</tr>
<tr>
<td>take</td>
<td>n:String</td>
<td>取出前 N个元素</td>
</tr>
<tr>
<td>saveAsTextFile</td>
<td>path:String</td>
<td>保存到文件</td>
</tr>
<tr>
<td>saveAsSequenceFile</td>
<td>path:String</td>
<td>保存成序列文件</td>
</tr>
<tr>
<td>saveAsObjectFile</td>
<td>path:String</td>
<td>Java and Scala</td>
</tr>
<tr>
<td>countByKey</td>
<td></td>
<td>按照key,统计每个key下value的个数</td>
</tr>
</tbody></table>
<h2 id="五、Spark核心API"><a href="#五、Spark核心API" class="headerlink" title="五、Spark核心API"></a>五、Spark核心API</h2><h2 id="六、Spark任务提交流程"><a href="#六、Spark任务提交流程" class="headerlink" title="六、Spark任务提交流程"></a>六、Spark任务提交流程</h2><h2 id="七、依赖"><a href="#七、依赖" class="headerlink" title="七、依赖"></a>七、依赖</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NarrowDependency:	子RDD的每个分区依赖于父RDD的少量分区。</span><br><span class="line">	 |</span><br><span class="line">	&#x2F; \</span><br><span class="line">	---</span><br><span class="line">	 |----	OneToOneDependency		&#x2F;&#x2F;父子RDD之间的分区存在一对一关系。</span><br><span class="line">	 |----	RangeDependency			&#x2F;&#x2F;父RDD的一个分区范围和子RDD存在一对一关系。</span><br><span class="line">	 |----	OneToOneDependency		&#x2F;&#x2F;父子RDD之间的分区存在一对一关系。</span><br><span class="line"></span><br><span class="line">ShuffleDependency					&#x2F;&#x2F;依赖，在shuffle阶段输出时的一种依赖。</span><br><span class="line"></span><br><span class="line">PruneDependency						&#x2F;&#x2F;在PartitionPruningRDD和其父RDD之间的依赖</span><br><span class="line">									&#x2F;&#x2F;子RDD包含了父RDD的分区子集。</span><br></pre></td></tr></table></figure></div>



<h2 id="八、持久化"><a href="#八、持久化" class="headerlink" title="八、持久化"></a>八、持久化</h2><h2 id="九、共享变量"><a href="#九、共享变量" class="headerlink" title="九、共享变量"></a>九、共享变量</h2><p>map(),filter()高级函数中访问的对象被串行化到各个节点。每个节点都有一份拷贝。变量值并不会回传到driver程序。spark通过广播变量和累加器实现共享变量。</p>
<h3 id="9-1-广播变量"><a href="#9-1-广播变量" class="headerlink" title="9.1 广播变量"></a>9.1 广播变量</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建广播变量</span></span><br><span class="line"><span class="keyword">val</span> bc1 = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">bc1.value</span><br></pre></td></tr></table></figure></div>



<h3 id="9-2-累加器"><a href="#9-2-累加器" class="headerlink" title="9.2 累加器"></a>9.2 累加器</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ac1 = sc.longaccumulator(<span class="string">"ac1"</span>)</span><br><span class="line">ac1.value</span><br><span class="line">sc.parell..(<span class="number">1</span> to <span class="number">10</span>).map(_ * <span class="number">2</span>).map(e=&gt;&#123;ac1.add(<span class="number">1</span>) ; e&#125;).reduce(_+_)</span><br><span class="line">ac1.value			<span class="comment">//10</span></span><br></pre></td></tr></table></figure></div>



<h2 id="十、Spark-SQL"><a href="#十、Spark-SQL" class="headerlink" title="十、Spark SQL"></a>十、Spark SQL</h2><h3 id="10-1-Scala-版"><a href="#10-1-Scala-版" class="headerlink" title="10.1 Scala 版"></a>10.1 Scala 版</h3><h4 id="10-1-1-创建-DataFrame-收据框"><a href="#10-1-1-创建-DataFrame-收据框" class="headerlink" title="10.1.1 创建 DataFrame 收据框"></a>10.1.1 创建 DataFrame 收据框</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建样例类</span></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">id:<span class="type">Int</span>, name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">创建数据，并转换成RDD</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">arr</span> </span>= <span class="type">Array</span>(<span class="string">"1,tom,12"</span>, <span class="string">"2,wang,13"</span>, <span class="string">"3,li,16"</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>,tom,<span class="number">12</span>, <span class="number">2</span>,wang,<span class="number">13</span>, <span class="number">3</span>,li,<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(arr)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">// 创建对象RDD（即将字符串RDD转换成一个对象）</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = rdd1.map(e =&gt; &#123;<span class="keyword">val</span> pars = e.split(<span class="string">","</span>); <span class="type">Person</span>(pars(<span class="number">0</span>).toInt, pars(<span class="number">1</span>), pars(<span class="number">2</span>).toInt)&#125;)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"><span class="comment">// 根据RDD创建DataFrame</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.createDataFrame(rdd2)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">1</span> more field]</span><br><span class="line"><span class="comment">// 打印表结构</span></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"> |-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- age: integer (nullable = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+---+----+---+                                                                  </span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  <span class="number">1</span>| tom| <span class="number">12</span>|</span><br><span class="line">|  <span class="number">2</span>|wang| <span class="number">13</span>|</span><br><span class="line">|  <span class="number">3</span>|  li| <span class="number">16</span>|</span><br><span class="line">+---+----+---+</span><br></pre></td></tr></table></figure></div>



<h4 id="10-1-2-创建临时视图"><a href="#10-1-2-创建临时视图" class="headerlink" title="10.1.2 创建临时视图"></a>10.1.2 创建临时视图</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 根据DataFrame在内存中创建一个临时的视图</span></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"><span class="comment">// 利用sparkSQL，根据内存中的视图，返回一个DataFrame</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df2 = spark.sql(<span class="string">"select * from persons"</span>)</span><br><span class="line">df2: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">1</span> more field]</span><br><span class="line">scala&gt; df2.show</span><br><span class="line">+---+----+---+                                                                  </span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  <span class="number">1</span>| tom| <span class="number">12</span>|</span><br><span class="line">|  <span class="number">2</span>|wang| <span class="number">13</span>|</span><br><span class="line">|  <span class="number">3</span>|  li| <span class="number">16</span>|</span><br><span class="line">+---+----+---+</span><br></pre></td></tr></table></figure></div>



<h4 id="10-1-3-查询数据"><a href="#10-1-3-查询数据" class="headerlink" title="10.1.3 查询数据"></a>10.1.3 查询数据</h4><p><strong>通过SQL查询</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 也可以添加条件查询，返回DataFrame</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df3 = spark.sql(<span class="string">"select * from persons where id &gt; 1"</span>)</span><br><span class="line">df3: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">1</span> more field]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df4 = spark.sql(<span class="string">"select * from persons where id &lt; 1"</span>)</span><br><span class="line">df4: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int, name: string ... <span class="number">1</span> more field]</span><br><span class="line"><span class="comment">// 然后创建视图</span></span><br><span class="line">scala&gt; df3.createOrReplaceTempView(<span class="string">"p1"</span>)</span><br><span class="line">scala&gt; df4.createOrReplaceTempView(<span class="string">"p2"</span>)</span><br><span class="line"><span class="comment">// 然后查询</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select * from p1 union select * from p2"</span>).show</span><br><span class="line">+---+----+---+                                                                  </span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  <span class="number">3</span>|  li| <span class="number">16</span>|</span><br><span class="line">|  <span class="number">2</span>|wang| <span class="number">13</span>|</span><br><span class="line">+---+----+---+</span><br></pre></td></tr></table></figure></div>

<p><strong>通过API查询</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 通过函数，返回Dataset（DataFram实际上就是Dataset）</span></span><br><span class="line">scala&gt; df3.union(df4)</span><br><span class="line">res8: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [id: int, name: string ... <span class="number">1</span> more field]</span><br><span class="line"><span class="comment">// 显示</span></span><br><span class="line">scala&gt; res8.show</span><br><span class="line">+---+----+---+</span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  <span class="number">2</span>|wang| <span class="number">13</span>|</span><br><span class="line">|  <span class="number">3</span>|  li| <span class="number">16</span>|</span><br><span class="line">+---+----+---+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他API</span></span><br><span class="line"><span class="comment">// 查询指定字段</span></span><br><span class="line">scala&gt; df.selectExpr(<span class="string">"id"</span>, <span class="string">"name"</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">| id|name|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">1</span>| tom|</span><br><span class="line">|  <span class="number">2</span>|wang|</span><br><span class="line">|  <span class="number">3</span>|  li|</span><br><span class="line">+---+----+</span><br><span class="line"></span><br><span class="line"><span class="comment">// where条件查询</span></span><br><span class="line">scala&gt; df.where(<span class="string">"name like 't%'"</span>).show</span><br><span class="line">+---+----+---+</span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  <span class="number">1</span>| tom| <span class="number">12</span>|</span><br><span class="line">+---+----+---+</span><br><span class="line"></span><br><span class="line"><span class="comment">// 聚合函数</span></span><br><span class="line">scala&gt; df.agg(sum(<span class="string">"age"</span>),max(<span class="string">"age"</span>)).show</span><br><span class="line">+--------+--------+</span><br><span class="line">|sum(age)|max(age)|</span><br><span class="line">+--------+--------+</span><br><span class="line">|      <span class="number">41</span>|      <span class="number">16</span>|</span><br><span class="line">+--------+--------+</span><br></pre></td></tr></table></figure></div>



<h3 id="10-2-Java-版"><a href="#10-2-Java-版" class="headerlink" title="10.2 Java 版"></a>10.2 Java 版</h3><p><strong>导入依赖</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>



<h4 id="10-2-1-处理-json-数据"><a href="#10-2-1-处理-json-数据" class="headerlink" title="10.2.1 处理 json 数据"></a>10.2.1 处理 json 数据</h4><p><strong>创建数据文件</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">json</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">1</span>,<span class="attr">"name"</span>:<span class="string">"tom1"</span>,<span class="attr">"age"</span>:<span class="number">11</span>&#125;</span><br><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">2</span>,<span class="attr">"name"</span>:<span class="string">"tom2"</span>,<span class="attr">"age"</span>:<span class="number">12</span>&#125;</span><br><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">3</span>,<span class="attr">"name"</span>:<span class="string">"tom3"</span>,<span class="attr">"age"</span>:<span class="number">13</span>&#125;</span><br><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">4</span>,<span class="attr">"name"</span>:<span class="string">"tom4"</span>,<span class="attr">"age"</span>:<span class="number">14</span>&#125;</span><br><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">5</span>,<span class="attr">"name"</span>:<span class="string">"tom5"</span>,<span class="attr">"age"</span>:<span class="number">15</span>&#125;</span><br><span class="line">&#123;<span class="attr">"id"</span>:<span class="number">6</span>,<span class="attr">"name"</span>:<span class="string">"tom6"</span>,<span class="attr">"age"</span>:<span class="number">16</span>&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>编写代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.directory.shared.kerberos.codec.apRep.actions.ApRepInit;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JsonFileIO class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/5 11:47</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonFileIO</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkSession sparkSession = SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .appName(<span class="string">"SQL-java-Json"</span>)</span><br><span class="line">                .config(<span class="string">"spark.master"</span>, <span class="string">"local"</span>)    <span class="comment">// 本地模式</span></span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取文件</span></span><br><span class="line">        Dataset&lt;Row&gt; dataset = sparkSession.read().json(<span class="string">"file:///d:/tmp/jsonSQL.dat"</span>);</span><br><span class="line">        dataset.show();</span><br><span class="line">        <span class="comment">// 创建临时视图</span></span><br><span class="line">        dataset.createOrReplaceTempView(<span class="string">"persons"</span>);</span><br><span class="line">        <span class="comment">// SQL</span></span><br><span class="line">        Dataset&lt;Row&gt; result = sparkSession.sql(<span class="string">"select * from persons where id &gt; 2"</span>);</span><br><span class="line">        result.show();</span><br><span class="line">        <span class="comment">// 写入文件(目录)</span></span><br><span class="line">        result.write().json(<span class="string">"file:///d:/tmp/jsonSQLOutDir"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// DataSet 转 RDD （可以进行一些数据操作！）</span></span><br><span class="line">        JavaRDD&lt;Row&gt; javaRDD = dataset.toJavaRDD();</span><br><span class="line">        javaRDD.filter(<span class="keyword">new</span> Function&lt;Row, Boolean&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Long id = (Long) row.getAs(<span class="string">"id"</span>);</span><br><span class="line">                <span class="keyword">if</span> (id &gt; <span class="number">3</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(row);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>输出结果</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 从文件读取的数据</span><br><span class="line">+---+---+----+</span><br><span class="line">|age| id|name|</span><br><span class="line">+---+---+----+</span><br><span class="line">| 11|  1|tom1|</span><br><span class="line">| 12|  2|tom2|</span><br><span class="line">| 13|  3|tom3|</span><br><span class="line">| 14|  4|tom4|</span><br><span class="line">| 15|  5|tom5|</span><br><span class="line">| 16|  6|tom6|</span><br><span class="line">+---+---+----+</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 经过SQL语句过滤的</span><br><span class="line">+---+---+----+</span><br><span class="line">|age| id|name|</span><br><span class="line">+---+---+----+</span><br><span class="line">| 13|  3|tom3|</span><br><span class="line">| 14|  4|tom4|</span><br><span class="line">| 15|  5|tom5|</span><br><span class="line">| 16|  6|tom6|</span><br><span class="line">+---+---+----+</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 最后输出到文件的</span><br><span class="line">&#123;&quot;age&quot;:13,&quot;id&quot;:3,&quot;name&quot;:&quot;tom3&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:14,&quot;id&quot;:4,&quot;name&quot;:&quot;tom4&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:15,&quot;id&quot;:5,&quot;name&quot;:&quot;tom5&quot;&#125;</span><br><span class="line">&#123;&quot;age&quot;:16,&quot;id&quot;:6,&quot;name&quot;:&quot;tom6&quot;&#125;</span><br></pre></td></tr></table></figure></div>



<h4 id="10-2-2-DataSet-转-RDD"><a href="#10-2-2-DataSet-转-RDD" class="headerlink" title="10.2.2 DataSet 转 RDD"></a>10.2.2 DataSet 转 RDD</h4><p><strong>完整代码在上面JSON代码里</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DataSet 转 RDD （可以进行一些数据操作！）</span></span><br><span class="line">JavaRDD&lt;Row&gt; javaRDD = dataset.toJavaRDD();</span><br><span class="line">javaRDD.filter(<span class="keyword">new</span> Function&lt;Row, Boolean&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Long id = (Long) row.getAs(<span class="string">"id"</span>);</span><br><span class="line">        <span class="keyword">if</span> (id &gt; <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Row&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(row);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></div>

<p><strong>输出结果</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[14,4,tom4]</span><br><span class="line">[15,5,tom5]</span><br><span class="line">[16,6,tom6]</span><br></pre></td></tr></table></figure></div>



<h4 id="10-2-3-处理-jdbc-数据"><a href="#10-2-3-处理-jdbc-数据" class="headerlink" title="10.2.3 处理 jdbc 数据"></a>10.2.3 处理 jdbc 数据</h4><p><strong>添加依赖</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Column;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JdbcIO class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/5 13:30</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JdbcIO</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkSession sparkSession = SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .appName(<span class="string">"SQL-java-jdbc"</span>)</span><br><span class="line">                .config(<span class="string">"spark.master"</span>, <span class="string">"local"</span>)    <span class="comment">// 本地模式</span></span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置数据库信息</span></span><br><span class="line">        String url = <span class="string">"jdbc:mysql:///test"</span>;</span><br><span class="line">        String table = <span class="string">"user"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.put(<span class="string">"user"</span>, <span class="string">"root"</span>);</span><br><span class="line">        prop.put(<span class="string">"password"</span>, <span class="string">"Bow1024"</span>);</span><br><span class="line">        prop.put(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        <span class="comment">// 读取数据库数据</span></span><br><span class="line">        Dataset&lt;Row&gt; dataset = sparkSession.read().jdbc(url, table, prop);</span><br><span class="line">        dataset.show();</span><br><span class="line">        <span class="comment">// 查询操作</span></span><br><span class="line">        Dataset&lt;Row&gt; result = dataset.select(<span class="keyword">new</span> Column(<span class="string">"id"</span>), <span class="keyword">new</span> Column(<span class="string">"name"</span>)).where(<span class="string">"id &gt; 1"</span>);</span><br><span class="line">        <span class="comment">// 写入数据库</span></span><br><span class="line">        result.write().jdbc(url, <span class="string">"sparkSQL"</span>, prop);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>输出结果</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 数据库读取的内容</span><br><span class="line">+---+--------+--------+----+</span><br><span class="line">| id|username|password|name|</span><br><span class="line">+---+--------+--------+----+</span><br><span class="line">|  1|zhangsan|     123|张三|</span><br><span class="line">|  2|    lisi|     123|李四|</span><br><span class="line">+---+--------+--------+----+</span><br></pre></td></tr></table></figure></div>

<p>数据库查询结果</p>
<p><a href="/postImages/jdbcConn.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/jdbcConn.png" class="lazyload"></a></p>
<h3 id="10-3-整合-Hive"><a href="#10-3-整合-Hive" class="headerlink" title="10.3 整合 Hive"></a>10.3 整合 Hive</h3><h4 id="10-3-1-linux环境"><a href="#10-3-1-linux环境" class="headerlink" title="10.3.1 linux环境"></a>10.3.1 linux环境</h4><ol>
<li><p>复制core-site.xml(hdfs) + hdfs-site.xml(hdfs) + hive-site.xml(hive)三个文件到spark/conf下。（分发到所有节点）</p>
</li>
<li><p>复制hive/lib下mysql驱动程序到/soft/spark/jars下（分发到所有节点）</p>
</li>
<li><p>启动spark-shell，指定启动模式</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-shell --master spark:&#x2F;&#x2F;s201:7077</span><br><span class="line"></span><br><span class="line"># 创建HIVE表</span><br><span class="line">scala&gt; spark.sql(&quot;CREATE TABLE IF NOT EXISTS mydb.sparkHive(id int, name string, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\n&#39; STORED AS TEXTFILE&quot;)</span><br><span class="line"></span><br><span class="line"># 本地创建数据文件</span><br><span class="line">1,tom1,11</span><br><span class="line">2,tom2,12</span><br><span class="line">3,tom3,13</span><br><span class="line">4,tom4,14</span><br><span class="line">5,tom5,15</span><br><span class="line"></span><br><span class="line"># 加载数据到HIVE表</span><br><span class="line">scala&gt; spark.sql(&quot;load data local inpath &#39;file:&#x2F;&#x2F;&#x2F;home&#x2F;wbw&#x2F;tmp&#x2F;data.txt&#39; into table mydb.sparkHive&quot;)</span><br><span class="line"></span><br><span class="line"># 查看HIVE表</span><br><span class="line">scala&gt; spark.sql(&quot;select * from mydb.sparkHive&quot;).show</span><br><span class="line">+----+----+----+                                                                </span><br><span class="line">|  id|name| age|</span><br><span class="line">+----+----+----+</span><br><span class="line">|   1|tom1|  11|</span><br><span class="line">|   2|tom2|  12|</span><br><span class="line">|   3|tom3|  13|</span><br><span class="line">|   4|tom4|  14|</span><br><span class="line">|   5|tom5|  15|</span><br><span class="line">|null|null|null|</span><br><span class="line">|null|null|null|</span><br><span class="line">+----+----+----+</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h4 id="10-3-2-IDEA环境（JAVA）"><a href="#10-3-2-IDEA环境（JAVA）" class="headerlink" title="10.3.2 IDEA环境（JAVA）"></a>10.3.2 IDEA环境（JAVA）</h4><ol>
<li><p>复制core-site.xml(hdfs) + hdfs-site.xml(hdfs) + hive-site.xml(hive)三个文件到resources目录下</p>
</li>
<li><p>编码</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkHive class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/5 15:59</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkHive</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkSession sparkSession = SparkSession</span><br><span class="line">                .builder()</span><br><span class="line">                .appName(<span class="string">"SQL-java-hive"</span>)</span><br><span class="line">                .config(<span class="string">"spark.master"</span>, <span class="string">"local"</span>)    <span class="comment">// 本地模式</span></span><br><span class="line">                .enableHiveSupport()				<span class="comment">// 这个是关键！</span></span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        Dataset&lt;Row&gt; dataset = sparkSession.sql(<span class="string">"select * from mydb.sparkHive"</span>);</span><br><span class="line">        dataset.show();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h4 id="10-3-3-可能遇到的问题"><a href="#10-3-3-可能遇到的问题" class="headerlink" title="10.3.3 可能遇到的问题"></a>10.3.3 可能遇到的问题</h4><p><strong>Spark2.3.0集成hive3.1.1遇到的一个坑HikariCP</strong></p>
<p><a href="https://blog.csdn.net/weixin_44166276/article/details/85088998" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44166276/article/details/85088998</a></p>
<p>TIP：我是修改了hive-site.xml中</p>
<ol>
<li><p><strong>datanucleus.connectionPoolingType</strong>改成<strong>dbcp</strong>。</p>
</li>
<li><p>然后关闭了版本验证<strong>hive.metastore.schema.verification</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<p><strong>IDEA下权限不足 Permission denied: user=BoWenWang, access=EXECUTE, inode=”/tmp”:wbw:supergroup:drwx——</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 哪个缺权限，就给哪个</span><br><span class="line">hdfs dfs -chmod 777 &#x2F;tmp</span><br></pre></td></tr></table></figure></div>





<h3 id="10-4-SQL查询引擎"><a href="#10-4-SQL查询引擎" class="headerlink" title="10.4 SQL查询引擎"></a>10.4 SQL查询引擎</h3><p>相当于嵌套了一层，通过JDBC的途径间接调用。</p>
<ol>
<li><p>启动spark集群(完全分布式-standalone)</p>
<pre><code>$&gt;/soft/spark/sbin/start-all.sh
   master        //201
   worker        //202 ~ 204</code></pre></li>
<li><p>创建hive的数据表在默认库下。</p>
<pre><code>$&gt;hive -e &quot;create table tt(id int,name string , age int) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos; stored as textfile&quot;</code></pre></li>
<li><p>加载数据到hive表中.</p>
<pre><code>$&gt;hive -e &quot;load data local inpath &apos;file:///home/centos/data.txt&apos; into table tt&quot;
   $&gt;hive -e &quot;select * from tt&quot;</code></pre></li>
<li><p>分发三个文件到所有worker节点</p>
</li>
<li><p>启动spark集群</p>
<pre><code>$&gt;soft/spark/sbin/start-all.sh</code></pre></li>
<li><p>启动spark-shell</p>
<pre><code>$&gt;spark-shell --master spark://s201:7077</code></pre></li>
<li><p>启动thriftserver服务器</p>
<pre><code>$&gt;start</code></pre></li>
</ol>
<h3 id="10-5-拓展和总结"><a href="#10-5-拓展和总结" class="headerlink" title="10.5 拓展和总结"></a>10.5 拓展和总结</h3><h4 id="10-5-1-使用场景"><a href="#10-5-1-使用场景" class="headerlink" title="10.5.1 使用场景"></a>10.5.1 使用场景</h4><p>处理结构化数据：</p>
<ul>
<li>即席/普通查询文件中的数据</li>
<li>对流数据处理（SparkStreaming）</li>
<li>使用SQL的方式处理ETL</li>
<li>对外部数据库进行查询</li>
</ul>
<h4 id="10-5-2-加载数据"><a href="#10-5-2-加载数据" class="headerlink" title="10.5.2 加载数据"></a>10.5.2 加载数据</h4><ul>
<li>直接加载数据到DataFrame中</li>
<li>可以加载数据到RDD然后做相应的转换</li>
<li>可以从本地或云端加载数据</li>
</ul>
<h4 id="10-5-3-DataFrame函数-和-SQL"><a href="#10-5-3-DataFrame函数-和-SQL" class="headerlink" title="10.5.3 DataFrame函数 和 SQL"></a>10.5.3 DataFrame函数 和 SQL</h4><ul>
<li><p>DataFrame = RDD + Schema：表格 = 数据 + 结构</p>
</li>
<li><p>DataFrame只是DataSet的Row类型的别名</p>
</li>
<li><p>DataFrame可以处理Text、JSON、parquet等（即外部数据源）</p>
</li>
<li><p>不管用DF的API还是SQL，最后生成的逻辑执行计划都是一样的，优化后效率都是一样的</p>
</li>
</ul>
<h4 id="10-5-4-Schema"><a href="#10-5-4-Schema" class="headerlink" title="10.5.4 Schema"></a>10.5.4 Schema</h4><ul>
<li>隐式：JSON、parquet、ORC等自动推导</li>
<li>显示：<ul>
<li>先创建RDD</li>
<li>然后创建一个schema</li>
<li>然后整合</li>
</ul>
</li>
</ul>
<h4 id="10-5-5-加载和保存结果"><a href="#10-5-5-加载和保存结果" class="headerlink" title="10.5.5 加载和保存结果"></a>10.5.5 加载和保存结果</h4><p>SaveMode：存在报错、Append追加、Overwrite覆盖、不理睬</p>
<p>spark.read.format(“json”).load(“file:///…/…”)</p>
<p>df.write.format(“parquet”).mode(“overwrite”).save(“file:///…/…”)</p>
<p>如果保存到数据库的话：saveAsTable</p>
<p>还可以分区写出去df.toDF.write.partitionBy(“year”,”month”).avor(“…”)</p>
<h4 id="10-5-6-SQL函数覆盖"><a href="#10-5-6-SQL函数覆盖" class="headerlink" title="10.5.6 SQL函数覆盖"></a>10.5.6 SQL函数覆盖</h4><p>支持了好多SQL语法</p>
<h4 id="10-5-7-复杂JSON处理"><a href="#10-5-7-复杂JSON处理" class="headerlink" title="10.5.7 复杂JSON处理"></a>10.5.7 复杂JSON处理</h4><p>数组情况</p>
<p>select name,nums[1] from json_table</p>
<p>多层嵌套可以用A.B</p>
<h4 id="10-5-8-外部数据源"><a href="#10-5-8-外部数据源" class="headerlink" title="10.5.8 外部数据源"></a>10.5.8 外部数据源</h4><ul>
<li><p>关系型数据库，需要JDBC jars</p>
</li>
<li><p>spark-packeages.org（就是format不一样.将原来JSON因为是自带的，可以直接写JSON，不然就要写对应的包。可以去那个网站上找）</p>
<p>spark.read.format(“com.databricks.spark.avro”).load(“…”)</p>
</li>
</ul>
<h2 id="十一、Spark-Streaming"><a href="#十一、Spark-Streaming" class="headerlink" title="十一、Spark Streaming"></a>十一、Spark Streaming</h2><h3 id="11-1-介绍"><a href="#11-1-介绍" class="headerlink" title="11.1 介绍"></a>11.1 介绍</h3><p>是spark core的扩展，针对实时数据流处理,具有可扩展、高吞吐量、容错。数据可以是来自于kafka,flume,tcpsocket,使用高级函数(map reduce filter ,join , windows),<br>处理的数据可以推送到database,hdfs,针对数据流处理可以应用到机器学习和图计算中。</p>
<p>内部，spark接受实时数据流，分成batch(分批次)进行处理，最终在每个batch终产生结果stream。</p>
<p><strong>discretized stream or DStream</strong><br>离散流,表示的是连续的数据流。<br>通过kafka、flume等输入数据流产生，也可以通过对其他DStream进行高阶变换产生。<br>在内部，DStream是表现为RDD序列。</p>
<p><strong>StreamingContext</strong></p>
<ol>
<li>启动上下文之后，不能启动新的流或者添加新的</li>
<li>上下文停止后不能restart.</li>
<li>同一JVM只有一个active的streamingcontext</li>
<li>停止streamingContext会一同stop掉SparkContext，如若只停止StreamingContext.ssc.stop(false|true);</li>
<li>SparkContext可以创建多个StreamingContext,创建新的之前停掉旧的。</li>
</ol>
<h3 id="11-2-快速入门-单词计数"><a href="#11-2-快速入门-单词计数" class="headerlink" title="11.2 快速入门(单词计数)"></a>11.2 快速入门(单词计数)</h3><p><strong>添加pom依赖</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>



<h4 id="11-2-1-Scala-本地"><a href="#11-2-1-Scala-本地" class="headerlink" title="11.2.1 Scala + 本地"></a>11.2.1 Scala + 本地</h4><p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingWordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 注意这个local[2]，线程数不能1个，因为要有1个用来接受数据，一个来处理</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"WC-Streaming-Scala"</span>)</span><br><span class="line">    <span class="comment">// 创建SparkStreaming上下文，批次时长是1秒</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建socket文本流（即数据源）</span></span><br><span class="line">    <span class="keyword">val</span> line = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据进行处理</span></span><br><span class="line">    <span class="keyword">val</span> words = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> count = pairs.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 要有输出，程序才能执行</span></span><br><span class="line">    count.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动作业</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 等待结束（不会停下来，只有调用stop()）</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>运行步骤</strong></p>
<ol>
<li><p>开启nc服务器（【安装教程】<a href="https://blog.csdn.net/weixin_38842096/article/details/85720559）" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38842096/article/details/85720559）</a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmd&gt; nc -lL -p 9999</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>启动sparkStreaming程序</p>
</li>
<li><p>在nc命令行中输入</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello spark streaming</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看IDEA控制台输出</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588831126000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(streaming,1)</span><br><span class="line">(spark,1)</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h4 id="11-2-2-Java-集群"><a href="#11-2-2-Java-集群" class="headerlink" title="11.2.2 Java + 集群"></a>11.2.2 Java + 集群</h4><p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkStreamingWordCountJava class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/7 14:03</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingWordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"WC-Streaming-Java"</span>).setMaster(<span class="string">"spark://s201:7077"</span>);</span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> JavaReceiverInputDStream&lt;String&gt; line = jsc.socketTextStream(<span class="string">"s201"</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = line.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = s.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(words).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; count = pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> integer + integer2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        count.print();</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>运行步骤</strong></p>
<ol>
<li><p>打jar包，上传导集群</p>
</li>
<li><p>在s201打开nc服务器</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -l -p 9999</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>提交作业</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.java.SparkStreamingWordCountJava Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>等待启动成功后，在NC中输入</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hello world count</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看运行窗口</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588833156000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(world,1)</span><br><span class="line">(count,1)</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="11-3-DStream-和-Receiver"><a href="#11-3-DStream-和-Receiver" class="headerlink" title="11.3 DStream 和 Receiver"></a>11.3 DStream 和 Receiver</h3><h4 id="11-3-1-DSteam"><a href="#11-3-1-DSteam" class="headerlink" title="11.3.1 DSteam"></a>11.3.1 DSteam</h4><ol>
<li></li>
</ol>
<h4 id="11-3-2-Receiver"><a href="#11-3-2-Receiver" class="headerlink" title="11.3.2 Receiver"></a>11.3.2 Receiver</h4><ol>
<li>介绍：Receiver是接受者，从source接受数据，存储在内存中共spark处理。</li>
<li>源<ul>
<li>基本源:fileSystem | socket,内置API支持。</li>
<li>高级源:kafka | flume | …，需要引入pom.xml依赖.</li>
</ul>
</li>
<li>注意：使用local模式时，不能使用一个线程.使用的local[n]，n需要大于receiver的个数。</li>
</ol>
<h3 id="11-4-Kafka-集成"><a href="#11-4-Kafka-集成" class="headerlink" title="11.4 Kafka 集成"></a>11.4 Kafka 集成</h3><ol>
<li><p>确保ZK集群开启</p>
</li>
<li><p>启动Kafka，在S202~S204分别运行</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$cd &#x2F;soft&#x2F;kafka</span><br><span class="line">$bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>创建kafka主题，查看主题列表</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$kafka-topics.sh --create --bootstrap-server s202:9092 --replication-factor 2 --partitions 2 --topic mytopic1</span><br><span class="line"># 查看主题列表</span><br><span class="line">$kafka-topics.sh --list --bootstrap-server s202:9092</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>导入依赖</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>
</li>
<li><p>编写代码</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkStreamingWordCountJava class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/7 14:03</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingWordCountJavaForKafka</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"WC-Streaming-Java-Kafka"</span>).setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Kafka连接配置</span></span><br><span class="line">        Map&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"s202:9092,s203:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>, <span class="string">"g6"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 订阅主题列表</span></span><br><span class="line">        Collection&lt;String&gt; topics = Collections.singletonList(<span class="string">"mytopic1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 数据源</span></span><br><span class="line">        <span class="keyword">final</span> JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; stream =</span><br><span class="line">                KafkaUtils.createDirectStream(</span><br><span class="line">                        jsc,</span><br><span class="line">                        LocationStrategies.PreferConsistent(),</span><br><span class="line">                        ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 压扁操作（消费数据源）</span></span><br><span class="line">        JavaDStream&lt;String&gt; words = stream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;ConsumerRecord&lt;String, String&gt;, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; consumerRecord)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// consumerRecord.key(): kafka消息由K-V组成，这里是控制台发送所以Key为null</span></span><br><span class="line">                String[] words = consumerRecord.value().split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(words).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; count = pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> integer + integer2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        count.print();</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>开启Kafka控制台生产者</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$kafka-console-producer.sh --broker-list s202:9092 --topic mytopic1</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>运行程序（Run）</p>
</li>
<li><p>在生产者控制台发送</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;hello spark streaming for kafka</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>观察IDEA控制台输出</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588835416000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(streaming,1)</span><br><span class="line">(kafka,1)</span><br><span class="line">(spark,1)</span><br><span class="line">(for,1)</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="11-5-状态更新"><a href="#11-5-状态更新" class="headerlink" title="11.5 状态更新"></a>11.5 状态更新</h3><h4 id="11-5-1-updateStateByKey"><a href="#11-5-1-updateStateByKey" class="headerlink" title="11.5.1 updateStateByKey"></a>11.5.1 updateStateByKey</h4><p>可以在指定的批次间隔内返回之前的全部历史数据，包括新增的，改变的和没有改变的。由于updateStateByKey在使用的时候一定要做checkpoint，当数据量过大的时候，checkpoint会占据庞大的数据量，会影响性能，效率不高。</p>
<p><strong>适用场景</strong></p>
<p>updateStateByKey可以用来统计历史数据。例如统计不同时间段用户平均消费金额，消费次数，消费总额，网站的不同时间段的访问量等指标。</p>
<h4 id="11-5-2-mapWithState"><a href="#11-5-2-mapWithState" class="headerlink" title="11.5.2 mapWithState"></a>11.5.2 mapWithState</h4><p>只返回变化后的key的值，这样做的好处是，我们可以只是关心那些已经发生的变化的key，对于没有数据输入，则不会返回那些没有变化的key的数据。这样的话，即使数据量很大，checkpoint也不会像updateStateByKey那样，占用太多的存储，效率比较高（再生产环境中建议使用这个）。</p>
<p><strong>适用场景</strong></p>
<p>mapWithState可以用于一些实时性较高，延迟较少的一些场景，例如你在某宝上下单买了个东西，付款之后返回你账户里的余额信息。</p>
<blockquote>
<p>版权声明：本文为CSDN博主「爱是与世界平行」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/an1090239782/article/details/102832444" target="_blank" rel="noopener">https://blog.csdn.net/an1090239782/article/details/102832444</a></p>
</blockquote>
<h4 id="11-5-3-代码示例"><a href="#11-5-3-代码示例" class="headerlink" title="11.5.3 代码示例"></a>11.5.3 代码示例</h4><p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.Optional;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkStreamingWordCountJava class</span></span><br><span class="line"><span class="comment"> * 状态更新</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/7 14:03</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingStatus</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"WC-Streaming-Status"</span>).setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> JavaReceiverInputDStream&lt;String&gt; line = jsc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = line.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = s.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(words).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; count = pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> integer + integer2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 打印当前窗口数据</span></span><br><span class="line">        count.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置检查点（使用updateStateByKey必须设置检查点目录）</span></span><br><span class="line">        jsc.checkpoint(<span class="string">"file:///d:/tmp/sparkCheckPoint"</span>);</span><br><span class="line">        <span class="comment">// 这里做状态更新</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; historyCount = count.updateStateByKey(<span class="keyword">new</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> integers 这里是相同的Key为一组进来接受处理</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> optional 这里是上一个旧的值</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title">call</span><span class="params">(List&lt;Integer&gt; integers, Optional&lt;Integer&gt; optional)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// 如果没有上一个值（即Null），那么取0</span></span><br><span class="line">                Integer newCounter = optional.orElse(<span class="number">0</span>);</span><br><span class="line">                <span class="comment">// 累加值</span></span><br><span class="line">                <span class="keyword">for</span> (Integer integer : integers) &#123;</span><br><span class="line">                    newCounter += integer;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> Optional.of(newCounter);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 打印状态更新值，即累计值</span></span><br><span class="line">        historyCount.print();</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>运行步骤</strong></p>
<ol>
<li><p>打开nc，间隔一段时间发送一次数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\XXX&gt;nc -lL -p 9999</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看IDEA控制台打印信息</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; count.print(); 当前窗口统计值</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588837904000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; historyCount.print(); 历史累积值</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588837904000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,4)</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="11-6-窗口化操作"><a href="#11-6-窗口化操作" class="headerlink" title="11.6 窗口化操作"></a>11.6 窗口化操作</h3><ul>
<li><p>batch interval : 批次的间隔。</p>
</li>
<li><p>windows length : 窗口长度,跨批次。是批次的整数倍。</p>
</li>
<li><p>slide interval : 滑动间隔,窗口计算的间隔时间，是批次interval的整倍数。</p>
</li>
</ul>
<p>比如：</p>
<p><strong>batch interval  = 2，windows length = 6，slide interval =4</strong></p>
<p>统计6秒内的热词，每2秒计算一次接收到的数据，每4秒刷新一次。</p>
<p><a href="/postImages/windows.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/windows.png" class="lazyload"></a></p>
<p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkStreamingWordCountJava class</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/7 14:03</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingWindows</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"WC-Streaming-Windows"</span>).setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次的间隔:每2秒计算一次接收到的数据</span></span><br><span class="line">        JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> JavaReceiverInputDStream&lt;String&gt; line = jsc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = line.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = s.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(words).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 窗口机制</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; count = pairs.reduceByKeyAndWindow(</span><br><span class="line">                <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> integer + integer2;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="keyword">new</span> Duration(<span class="number">6</span> * <span class="number">1000</span>), <span class="comment">// 窗口长度</span></span><br><span class="line">                <span class="keyword">new</span> Duration(<span class="number">4</span> * <span class="number">1000</span>)  <span class="comment">// 滑动窗口长度</span></span><br><span class="line">        );</span><br><span class="line">        count.print();</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>



<p><strong>运行过程</strong></p>
<ol>
<li><p>开启nc</p>
</li>
<li><p>运行程序</p>
</li>
<li><p>发送数据（每隔1秒发一次）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\XXX&gt;nc -lL -p 9999</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看输出结果（每次输出间隔 4s = 4000ms，每2秒计算一次，用[]表示2秒的数据，显示6秒内的数据）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839240000 ms </span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839244000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(2,1)</span><br><span class="line">(1,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839248000 ms	</span><br><span class="line">-------------------------------------------</span><br><span class="line">(4,1)</span><br><span class="line">(6,1)</span><br><span class="line">(2,1)</span><br><span class="line">(5,1)</span><br><span class="line">(3,1)</span><br><span class="line">(1,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839252000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(8,1)</span><br><span class="line">(6,1)</span><br><span class="line">(7,1)</span><br><span class="line">(5,1)</span><br><span class="line">(9,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839256000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(12,1)</span><br><span class="line">(13,1)</span><br><span class="line">(9,1)</span><br><span class="line">(11,1)</span><br><span class="line">(10,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588839260000 ms</span><br><span class="line">13</span><br><span class="line">-------------------------------------------</span><br><span class="line">(13,1)</span><br></pre></td></tr></table></figure></div>





</li>
</ol>
<h3 id="11-7-容错处理"><a href="#11-7-容错处理" class="headerlink" title="11.7 容错处理"></a>11.7 容错处理</h3><h4 id="11-7-1-生产环境中spark-streaming的job的注意事项"><a href="#11-7-1-生产环境中spark-streaming的job的注意事项" class="headerlink" title="11.7.1 生产环境中spark streaming的job的注意事项"></a>11.7.1 生产环境中spark streaming的job的注意事项</h4><ul>
<li>避免单点故障</li>
<li>Driver：驱动,运行用户编写的程序代码的主机。</li>
<li>Excutors：执行的spark driver提交的job,内部含有附加组件比如receiver。receiver接受数据并以block方式保存在memory中，同时，将数据块复制到其他executor中，已备于容错。每个批次末端会形成新的DStream，交给下游处理。如果receiver故障，其他执行器中的receiver会启动进行数据的接收。</li>
</ul>
<p><a href="/postImages/fail.png" data-fancybox="group" data-caption="avatar" class="fancybox"><img alt="avatar" title="avatar" data-src="/postImages/fail.png" class="lazyload"></a></p>
<h4 id="11-7-2-spark-streaming中的容错实现"><a href="#11-7-2-spark-streaming中的容错实现" class="headerlink" title="11.7.2 spark streaming中的容错实现"></a>11.7.2 spark streaming中的容错实现</h4><p>如果executor故障，所有未被处理的数据都会丢失，解决办法可以通过wal(hbase,hdfs/WALs)方式将数据预先写入到hdfs或者s3.</p>
<p>如果Driver故障，driver程序就会停止，所有executor都是丢失连接，停止计算过程。解决办法需要配置和编程。</p>
<p><strong>流程</strong></p>
<ol>
<li><p>配置Driver程序自动重启，使用特定的clustermanager实现。</p>
</li>
<li><p>重启时，从宕机的地方进行重启，通过检查点机制可以实现该功能。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 设置检查点目录可以是本地，可以是hdfs.</span><br><span class="line">jsc.checkpoint(&quot;d:&#x2F;&#x2F;....&quot;);</span><br><span class="line">&#x2F;&#x2F; 不再使用new方式创建SparkStreamContext对象，而是通过工厂方式.JavaStreamingContext.getOrCreate()方法创建上下文对象,首先会检查检查点目录，看是否有job运行，没有就new新的。</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<p><strong>代码</strong></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.Optional;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function0;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SparkStreamingRestart class</span></span><br><span class="line"><span class="comment"> * 容错处理</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> BoWenWang</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2020/5/7 17:22</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingRestart</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 上下文工厂函数，用来创建上下文</span></span><br><span class="line">        Function0&lt;JavaStreamingContext&gt; contextFactory = <span class="keyword">new</span> Function0&lt;JavaStreamingContext&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> JavaStreamingContext <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="comment">// 这里复用了之前的更新状态内容</span></span><br><span class="line">                SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"WC-Streaming-ReStart"</span>).setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">                JavaStreamingContext jsc = <span class="keyword">new</span> JavaStreamingContext(sparkConf, Seconds.apply(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">                <span class="keyword">final</span> JavaReceiverInputDStream&lt;String&gt; line = jsc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">                JavaDStream&lt;String&gt; words = line.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] words = s.split(<span class="string">" "</span>);</span><br><span class="line">                        <span class="keyword">return</span> Arrays.asList(words).iterator();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">                JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">                JavaPairDStream&lt;String, Integer&gt; count = pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer integer, Integer integer2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> integer + integer2;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 这里做状态更新</span></span><br><span class="line">                JavaPairDStream&lt;String, Integer&gt; historyCount = count.updateStateByKey(<span class="keyword">new</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="comment">/**</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> integers 这里是相同的Key为一组进来接受处理</span></span><br><span class="line"><span class="comment">                     * <span class="doctag">@param</span> optional 这里是上一个旧的值</span></span><br><span class="line"><span class="comment">                     */</span></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title">call</span><span class="params">(List&lt;Integer&gt; integers, Optional&lt;Integer&gt; optional)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        Integer newCounter = optional.orElse(<span class="number">0</span>);</span><br><span class="line">                        <span class="keyword">for</span> (Integer integer : integers) &#123;</span><br><span class="line">                            newCounter += integer;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> Optional.of(newCounter);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">                historyCount.print();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 设置检查点（要与下面的路径相同，启动的时候先检查该目录，有问题的时候也是写入到该目录）</span></span><br><span class="line">                jsc.checkpoint(<span class="string">"file:///d:/tmp/ReStartCheckPoint"</span>);</span><br><span class="line">                <span class="keyword">return</span> jsc;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每次启动的时候，先检查检查点，如果之间有断开，那么重启。否则新建一个上下文</span></span><br><span class="line">        JavaStreamingContext context = JavaStreamingContext.getOrCreate(<span class="string">"file:///d:/tmp/ReStartCheckPoint"</span>,</span><br><span class="line">                contextFactory);</span><br><span class="line"></span><br><span class="line">        context.start();</span><br><span class="line">        context.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<p><strong>运行步骤</strong></p>
<ol>
<li><p>开启NC</p>
</li>
<li><p>运行程序</p>
</li>
<li><p>发送数据</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Users\BoWenWang&gt;nc -lL -p 9999</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br><span class="line">hello</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>观察控制台输出</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588844410000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,5)</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>直接杀掉程序（日志）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO CheckpointWriter: Deleting file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844650000</span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO CheckpointWriter: Checkpoint for time 1588844660000 ms saved to file &#39;file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844660000&#39;, took 4337 bytes and 23 ms</span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO DStreamGraph: Clearing checkpoint data for time 1588844660000 ms</span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO DStreamGraph: Cleared checkpoint data for time 1588844660000 ms</span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO ReceivedBlockTracker: Deleting batches: 1588844638000 ms</span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO FileBasedWriteAheadLog_ReceivedBlockTracker: Attempting to clear 0 old log files in file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;receivedBlockMetadata older than 1588844640000: </span><br><span class="line">20&#x2F;05&#x2F;07 17:44:20 INFO InputInfoTracker: remove old batch metadata: 1588844638000 ms</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>重启程序（检查日志）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Using Spark&#39;s default log4j profile: org&#x2F;apache&#x2F;spark&#x2F;log4j-defaults.properties</span><br><span class="line">20&#x2F;05&#x2F;07 17:45:48 INFO CheckpointReader: Checkpoint files found: file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844660000,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844660000.bk,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844658000,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844658000.bk,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844656000,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844656000.bk,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844654000,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844654000.bk,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844652000,file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844652000.bk</span><br><span class="line">20&#x2F;05&#x2F;07 17:45:48 INFO CheckpointReader: Attempting to load checkpoint from file file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844660000</span><br><span class="line">20&#x2F;05&#x2F;07 17:45:50 INFO Checkpoint: Checkpoint for time 1588844660000 ms validated</span><br><span class="line">20&#x2F;05&#x2F;07 17:45:50 INFO CheckpointReader: Checkpoint successfully loaded from file file:&#x2F;d:&#x2F;tmp&#x2F;ReStartCheckPoint&#x2F;checkpoint-1588844660000</span><br><span class="line">20&#x2F;05&#x2F;07 17:45:50 INFO CheckpointReader: Checkpoint was generated at time 1588844660000 ms</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>观察控制台输出（看到恢复了）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 1588844660000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,5)</span><br></pre></td></tr></table></figure></div>




</li>
</ol>
<h3 id="11-8-将结果写入MySQL"><a href="#11-8-将结果写入MySQL" class="headerlink" title="11.8 将结果写入MySQL"></a>11.8 将结果写入MySQL</h3><p>利用11.2.1的代码，其中count.print()替换成输出到数据库即可：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 先定义一个函数获取连接</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createConnextion</span></span>() = &#123;</span><br><span class="line">    <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">    <span class="type">DriverManager</span>.getConnection(<span class="string">"url"</span>, <span class="string">"username"</span>, <span class="string">"password"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 添加新代码</span></span><br><span class="line">count.print()</span><br><span class="line">count.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">    rdd.foreachPartition(partitionOfRecords =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> connection = createConnextion()</span><br><span class="line">        partitionOfRecords.foreach(record =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> sql = <span class="string">"insert into ..."</span></span><br><span class="line">            connection.createStatement().execute(sql)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></div>



<h2 id="十二、SparkApp-部署模式"><a href="#十二、SparkApp-部署模式" class="headerlink" title="十二、SparkApp 部署模式"></a>十二、SparkApp 部署模式</h2><p>决定spark作业入口程序的地方，Driver驱动。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit --class xxx xx.jar --deploy-mode (client | cluster)</span><br></pre></td></tr></table></figure></div>

<p>–deploy-mode：指定是否部署driver程序在worker节点上还是在client主机上。不论哪种方式，rdd的运算都在worker执行</p>
<h3 id="12-1-闭包"><a href="#12-1-闭包" class="headerlink" title="12.1 闭包"></a>12.1 闭包</h3><p>RDD,resilient distributed dataset,弹性(容错)分布式数据集。</p>
<p>分区列表,function,dep Option(分区类, Pair[Key,Value]),首选位置。</p>
<p>运行job时，spark将rdd打碎变换成task,每个task由一个executor执行。执行之前，spark会进行task的闭包(closure)计算。闭包是指针对executor可见的变量和方法,以备在rdd的foreach中进行计算。闭包就是串行化后并发送给每个executor.</p>
<p><strong>local</strong>模式下，所有spark程序运行在同一JVM中，共享对象，counter（变量）是可以累加的。原因是所有executor指向的是同一个引用。</p>
<p><strong>cluster</strong>模式下，不可以，counter是闭包处理的。每个节点对driver上的counter是不可见的。只能看到自己内部串行化的counter副本。</p>
<h3 id="12-2-client"><a href="#12-2-client" class="headerlink" title="12.2 client"></a>12.2 client</h3><p>driver运行在client主机上。client可以不在cluster中。</p>
<p><strong>验证</strong></p>
<ol>
<li><p>启动spark集群</p>
</li>
<li><p>编程</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">scala</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.wangbowen.spark.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.&#123;<span class="type">InetAddress</span>, <span class="type">Socket</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SubmitDeployMode</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 打印消息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printfInfo</span></span>(str:<span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ip = <span class="type">InetAddress</span>.getLocalHost.getHostAddress</span><br><span class="line">    <span class="keyword">val</span> socket = <span class="keyword">new</span> <span class="type">Socket</span>(<span class="string">"192.168.174.205"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> out = socket.getOutputStream</span><br><span class="line">    out.write((ip + <span class="string">": "</span> + str + <span class="string">"\r\n"</span>).getBytes())</span><br><span class="line">    out.flush()</span><br><span class="line">    socket.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Submit-Deploy-Mode"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    printfInfo(<span class="string">"Driver running on this node!"</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2 = rdd.map(num =&gt; &#123;</span><br><span class="line">      printfInfo(<span class="string">"map =&gt; "</span> + num)</span><br><span class="line">      num * <span class="number">2</span></span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> count = rdd2.reduce((num1, num2) =&gt; &#123;</span><br><span class="line">      printfInfo(<span class="string">"reduce =&gt; "</span> + num1 + <span class="string">","</span> + num2)</span><br><span class="line">      num1 + num2</span><br><span class="line">    &#125;)</span><br><span class="line">    printfInfo(<span class="string">"count: "</span> + count)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>随便找一台机子，开启nc（这里选s205）。如果没装。用命令 <strong>$sudo yum install nc -y</strong> 安装。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>打jar包上传到s201并执行</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.scala.SubmitDeployMode --master spark:&#x2F;&#x2F;s201:7077 --deploy-mode client Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>

<p>打印结果</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.174.201: Driver running on this node!	# 入口程序在s201上</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 1</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 2</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 2,4</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 3</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 6,6</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 4</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 5</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 8,10</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 6</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 18,12</span><br><span class="line">192.168.174.201: reduce &#x3D;&gt; 12,30</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 7</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 8</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 14,16</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 9</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 30,18</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 10</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 48,20</span><br><span class="line">192.168.174.201: reduce &#x3D;&gt; 42,68</span><br><span class="line">192.168.174.201: count: 110						# 最终结果返回client节点s201</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>将jar包上传到s202并执行</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.scala.SubmitDeployMode --master spark:&#x2F;&#x2F;s201:7077 --deploy-mode client Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>

<p>打印结果</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.174.202: Driver running on this node!	# 入口程序在s201上</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 1</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 2</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 2,4</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 3</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 6,6</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 4</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 5</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 8,10</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 7</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 6</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 8</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 18,12</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 14,16</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 9</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 30,18</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 10</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 48,20</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 12,68</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 80,30</span><br><span class="line">192.168.174.202: count: 110						# 最终结果返回client节点s202</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="12-3-cluster"><a href="#12-3-cluster" class="headerlink" title="12.3 cluster"></a>12.3 cluster</h3><p>driver程序提交给spark cluster的某个worker节点来执行。worker是cluster中的一员。导出的jar需要放置到所有worker节点都可见的位置(如hdfs)才可以。</p>
<ol>
<li><p>上传jar包到hdfs</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$hdfs dfs -mkdir &#x2F;demoJars</span><br><span class="line">$hdfs dfs -put Spark-1.0-SNAPSHOT.jar &#x2F;demoJars</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>提交任务（注意模式的改变deply-mdoe和hdfs路径下的jar）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-submit --class cn.wangbowen.spark.scala.SubmitDeployMode --master spark:&#x2F;&#x2F;s201:7077 --deploy-mode cluster hdfs:&#x2F;&#x2F;mycluster&#x2F;demoJars&#x2F;Spark-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>查看结果（发现在s201上提交，结果Driver在s202）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.174.202: Driver running on this node!</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 1</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 2</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 2,4</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 3</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 6,6</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 7</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 8</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 14,16</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 9</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 30,18</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 10</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 48,20</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 12,68</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 4</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 5</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 8,10</span><br><span class="line">192.168.174.204: map &#x3D;&gt; 6</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 18,12</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 80,30</span><br><span class="line">192.168.174.202: count: 110</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>再次提交（发现Driver又变了，在s204上）</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.174.204: Driver running on this node!</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 1</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 2</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 2,4</span><br><span class="line">192.168.174.203: map &#x3D;&gt; 3</span><br><span class="line">192.168.174.203: reduce &#x3D;&gt; 6,6</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 4</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 5</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 8,10</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 6</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 18,12</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 7</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 8</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 14,16</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 9</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 30,18</span><br><span class="line">192.168.174.202: map &#x3D;&gt; 10</span><br><span class="line">192.168.174.202: reduce &#x3D;&gt; 48,20</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 30,68</span><br><span class="line">192.168.174.204: reduce &#x3D;&gt; 98,12</span><br><span class="line">192.168.174.204: count: 110</span><br></pre></td></tr></table></figure></div>




</li>
</ol>
<h2 id="十三、Spark集成Hive查询Hbase"><a href="#十三、Spark集成Hive查询Hbase" class="headerlink" title="十三、Spark集成Hive查询Hbase"></a>十三、Spark集成Hive查询Hbase</h2><p>假设已经按照之前的步骤HIVE集成了HBASE了。此时HBase中有一个表有两条记录，而Hive关联了HBase的表，且表名为t1。</p>
<h3 id="13-1-local-模式-spark-shell"><a href="#13-1-local-模式-spark-shell" class="headerlink" title="13.1 local 模式 + spark-shell"></a>13.1 local 模式 + spark-shell</h3><ol>
<li><p>复制hive的hive-hbase-handler-2.1.0.jar文件到spark/jars目录下。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;hive&#x2F;lib]$cp hive-hbase-handler-3.1.2.jar &#x2F;soft&#x2F;spark&#x2F;jars&#x2F;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>复制hive/下的metrics的jar文件到spark下</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&gt;cd &#x2F;soft&#x2F;hive&#x2F;lib</span><br><span class="line">$&gt;ls | grep metrics | cp &#96;xargs&#96; &#x2F;soft&#x2F;spark&#x2F;jars</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>由于之间hive集成hbase时候修改了hive-site.xml这里，重新导入</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$cp hive-site.xml &#x2F;soft&#x2F;spark&#x2F;conf&#x2F;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>拷贝hbase的包到spark/jars下<strong>（这部有争议，先跳过，如果有问题先看是不是13.4中的，没办法才全部导入hbase的相关包）</strong></p>
</li>
<li><p>启动spark-shell 本地模式测试</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-shell --master local[4]</span><br><span class="line">$scala&gt;spark.sql(&quot;select * from t1&quot;).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">+----+----+---+                                                 </span><br><span class="line">| key|name| id|</span><br><span class="line">+----+----+---+</span><br><span class="line">|row1| tom|100|</span><br><span class="line">|row2|toms| 18|</span><br><span class="line">+----+----+---+</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>如果报错，可能是版本验证问题和数据库连接问题。跟之前一样参考<strong>10.3.3</strong>。其他缺包等问题参考<strong>13.4</strong></p>
</li>
</ol>
<h3 id="13-2-standalone-模式-spark-shell"><a href="#13-2-standalone-模式-spark-shell" class="headerlink" title="13.2 standalone 模式 + spark-shell"></a>13.2 standalone 模式 + spark-shell</h3><ol>
<li><p>在spark集群上分发<strong>13.1 （1）</strong>模式下所有需要的jar包。</p>
</li>
<li><p>启动standalone模式，spark集群。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wbw@s201 &#x2F;soft&#x2F;spark&#x2F;sbin]$.&#x2F;start-all.sh</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>启动spark-shell</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$spark-shell --master spark:&#x2F;&#x2F;s201:7077</span><br><span class="line">scala&gt; spark.sql(&quot;select * from t1&quot;).show</span><br><span class="line"></span><br><span class="line">+----+----+---+                                                                 </span><br><span class="line">| key|name| id|</span><br><span class="line">+----+----+---+</span><br><span class="line">|row1| tom|100|</span><br><span class="line">|row2|toms| 18|</span><br><span class="line">+----+----+---+</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h3 id="13-3-IDEA编程访问"><a href="#13-3-IDEA编程访问" class="headerlink" title="13.3 IDEA编程访问"></a>13.3 IDEA编程访问</h3><ol>
<li><p>导依赖</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">xml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-hbase-handler<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

<p>MAVEN发生错误？无法打包？</p>
<p>不过去掉hive-hbase-handler后，打包。在standalone下可以运行。</p>
</li>
</ol>
<h3 id="13-4-整合常见问题"><a href="#13-4-整合常见问题" class="headerlink" title="13.4 整合常见问题"></a>13.4 整合常见问题</h3><p><a href="https://www.wandouip.com/t5i62606/" target="_blank" rel="noopener">https://www.wandouip.com/t5i62606/</a></p>
<ol>
<li><p>问题一</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark 执行时报java.lang.ClassNotFoundException: org.apache.htrace.core.HTraceConfiguration</span><br></pre></td></tr></table></figure></div>

<p>解决方案</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将Hadoop中的htrace-core4-4.1.0-incubating.jar放入spark&#x2F;jars下：</span><br><span class="line"></span><br><span class="line">[wbw@s201 &#x2F;soft&#x2F;hadoop-3.1.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib]$cp htrace-core4-4.1.0-incubating.jar &#x2F;soft&#x2F;spark&#x2F;jars&#x2F;</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>问题二</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark 执行时报java.lang.ClassNotFoundException: org.apache.hadoop.hbase.util.Bytes</span><br></pre></td></tr></table></figure></div>

<p>解决方案</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将hbase&#x2F;lib下的hbase-common-2.2.4.jar放到spark&#x2F;jars下</span><br></pre></td></tr></table></figure></div>



</li>
</ol>
<h2 id="十四、优化"><a href="#十四、优化" class="headerlink" title="十四、优化"></a>十四、优化</h2><h3 id="14-1-存储格式的选择"><a href="#14-1-存储格式的选择" class="headerlink" title="14.1 存储格式的选择"></a>14.1 存储格式的选择</h3><p>在sparkSQL中可以使用 <code>parquet</code> ：一种文件格式类型，可以按照这个读取写出文件。</p>
<h3 id="14-2-压缩格式"><a href="#14-2-压缩格式" class="headerlink" title="14.2 压缩格式"></a>14.2 压缩格式</h3><h3 id="14-3-代码优化"><a href="#14-3-代码优化" class="headerlink" title="14.3 代码优化"></a>14.3 代码优化</h3><ol>
<li><p>选择高性能算子</p>
<p>在将数据写入到数据库中的时候</p>
<p>DataFrame变量.foreachPartition</p>
<p>每一次对每一个分区进行插入。不要一条记录插一次</p>
<p>先关掉自动提交，然后批量插入。最后commit。</p>
</li>
</ol>
<ol start="2">
<li><p>复用已有数据</p>
<p>通用的dataframe，可以XXX.cache()，不用了再XXX.unpersist(ture)</p>
</li>
</ol>
<h3 id="14-4-参数优化"><a href="#14-4-参数优化" class="headerlink" title="14.4 参数优化"></a>14.4 参数优化</h3><ol>
<li><p>并行度</p>
<p>spark.sql.shuffle.partitions</p>
</li>
<li><p>分区字段类型推测</p>
<p>spark.sql.sources.partionColumnTypeInference.enabled</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">IT小王</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wangbowen.cn/2020/04/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://wangbowen.cn/2020/04/28/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wangbowen.cn">IT小王</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark    </a></div><div class="post_share"><div class="social-share" data-image="/postImages/SparkCover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/11/JAVA%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E5%8D%B7I%EF%BC%88%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%BB%93%E6%9E%84%EF%BC%89/"><img class="prev_cover lazyload" data-src="/postImages/JavaCover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>JAVA核心技术卷I（基本程序设计结构）</span></div></a></div><div class="next-post pull_right"><a href="/2020/04/23/CentOS7%E6%90%AD%E5%BB%BASamba/"><img class="next_cover lazyload" data-src="/postImages/LinuxCover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>CentOS7搭建Samba</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'iYYoev4NDSyr3HqlY1VVCTIV-gzGzoHsz',
  appKey:'Hli8dWTFUKrXj9FnhnSQUvRe',
  placeholder:'快来评论吖！ヾﾉ≧∀≦)o',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By IT小王</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="icp"><a href="http://www.beian.miit.gov.cn" target="_blank" rel="noopener"><img class="icp-icon" src="/img/icp.png"><span>闽ICP备18027071号-1</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>